# Дообучение BERT на данных по токсичности

Имеется набор текстов на русском языке из 140 тысяч строк. Тексты - посты и комментарии из российских соцсетей. Тексты были предварительно размечены вручную на наличие токсичности по нескольким категориям: не токсично, мат/нецензурное, грубое высказывание, дискриминация, оскорбление, угроза.

Цель: Использовать одну из предобученных моделей на основе Bert, дообучить её на имеющихся данных, добиться качества предсказаний по имеющимся данным roc_auc не менее 0.9.

В качестве предобученной модели используем модель [*rubert-tiny-toxicity*](https://huggingface.co/cointegrated/rubert-tiny-toxicity), опубликованную на [*huggingface*](https://huggingface.co/). Для дообучения модели будем используем объект класса transformers.Trainer. Обучаем на GPU, 15 эпох, learning_rate - 1e-5.

Описание данных: Данные представляют собой набор текстов, каждому из которых соответствуют значения трех столбцов (mark1, mark2, mark3). В каждом столбце с оценками может быть проставлена одна из категорий токсичности (значение категории прописано в виде строки) или "0", если текст токсичности не содержит. Таким образом, каждому тексту может быть присвоено несколько категорий. Таким образом, задача сводится к классификации с несколькими метками (multi-label classification).

Ход работы:
1. Загрузка данных и библиотек.
2. Загрузка модели.
3. Предобработка данных:
   * формирование вектора меток;
   * исследование дисбаланса меток;
   * очистка текста.
4. Дообучение модели:
   * проработка дисбаланса меток;
   * токенизация;
   * дообучение.
5. Тестирование (отдельный блокнот) 

Работа представлена в двух файлах
1. [Дообучение](https://github.com/agamai/Portfolio/blob/main/Toxicity_rus/my_toxicity_15ep_final.ipynb)
2. [Тестирование](https://github.com/agamai/Portfolio/blob/main/Toxicity_rus/my_toxicity_15ep_final_test.ipynb)
