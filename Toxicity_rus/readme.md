# Дообучение BERT на данных по токсичности

Имеется набор текстов на русском языке из 140 тысяч строк. Тексты - посты и комментарии из российских соцсетей. Тексты были предварительно размечены вручную на наличие токсичности по нескольким категориям: не токсично, мат/нецензурное, грубое высказывание, дискриминация, оскорбление, угроза.

Цель: Использовать одну из предобученных моделей на основе Bert, дообучить её на имеющихся данных, добиться качества предсказаний по имеющимся данным roc_auc не менее 0.9.

В качестве предобученной модели будем использовать модель rubert-tiny-toxicity, опубликованную на huggingface. Для дообучения модели будем использовать объект класса transformers.Trainer. Обучать будем на GPU, 15 эпох, learning_rate - 1e-5.

Описание данных: Данные представляют собой набор текстов, каждому из которых соответствуют значения трех столбцов (mark1, mark2, mark3). В каждом столбце с оценками может быть проставлена одна из категорий токсичности (значение категории прописано в виде строки) или "0", если текст токсичности не содержит. Таким образом, каждому тексту может быть присвоено несколько категорий. Таким образом, задача сводится к классификации с несколькими метками (multi-label classification).

Ход работы:

Загрузка данных и библиотек.
Загрузка модели.
Предобработка данных:
формирование вектора меток;
исследование дисбаланса меток;
очистка текста.
Дообучение модели:
проработка дисбаланса меток;
токенизация;
дообучение.
